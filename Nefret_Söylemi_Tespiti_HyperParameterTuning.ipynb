{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YZUP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XYYDvoskkE61"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRjxtOirHW3o",
    "outputId": "986459b8-a72e-4e20-f4c0-f1543c5199f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nezih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Türkçe stop kelimeler\n",
    "stop_words = set(stopwords.words('turkish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0eJSTTYnkJQd"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 250\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 60000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BQVuQrZNkPn9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('troff-v1.0.tsv', delimiter='\\t')\n",
    "\n",
    "# Metin sütununu küçük harfe çevirin\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Noktalama işaretleri ve özel karakterleri kaldırın\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Sayıları kaldırın\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Gereksiz boşlukları kaldırın\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# Stop kelimeleri kaldırın\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Yinelenen kayıtları kaldırın\n",
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 'non' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH56JwLeKrrQ",
    "outputId": "2fc3148b-51f9-4ded-d4a7-16bab0fe29ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35236"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oaLaaqhNkUPd"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('train.csv')\n",
    "\n",
    "# Metin sütununu küçük harfe çevirin\n",
    "df2['text'] = df2['text'].str.lower()\n",
    "\n",
    "# Noktalama işaretleri ve özel karakterleri kaldırın\n",
    "df2['text'] = df2['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Sayıları kaldırın\n",
    "df2['text'] = df2['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Gereksiz boşlukları kaldırın\n",
    "df2['text'] = df2['text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# Stop kelimeleri kaldırın\n",
    "df2['text'] = df2['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Yinelenen kayıtları kaldırın\n",
    "df2.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LllHRPNeKw1C",
    "outputId": "1d833774-6b40-4b01-c6ee-2cb98bfae54d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42349"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AeGAzoGUImce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iyi partili türkkan milyon suriyeli gelecek</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>başkan erdoğandan suriyeli mülteci mesajı dest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arslan bulut yazdı metrekarelik konaklar yaptı...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>güzel bir amaçla yola çıkmış insan askerini po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kimin parası türk halkının parası peki bizim k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0        iyi partili türkkan milyon suriyeli gelecek      0\n",
       "1  başkan erdoğandan suriyeli mülteci mesajı dest...      0\n",
       "2  arslan bulut yazdı metrekarelik konaklar yaptı...      1\n",
       "3  güzel bir amaçla yola çıkmış insan askerini po...      0\n",
       "4  kimin parası türk halkının parası peki bizim k...      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('train2.csv')\n",
    "\n",
    "df3 = df3[['tweet','label']]\n",
    "\n",
    "df3.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "# Metin sütununu küçük harfe çevirin\n",
    "df3['tweet'] = df3['tweet'].str.lower()\n",
    "\n",
    "# Noktalama işaretleri ve özel karakterleri kaldırın\n",
    "df3['tweet'] = df3['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Sayıları kaldırın\n",
    "df3['tweet'] = df3['tweet'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Gereksiz boşlukları kaldırın\n",
    "df3['tweet'] = df3['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# Stop kelimeleri kaldırın\n",
    "df3['tweet'] = df3['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Yinelenen kayıtları kaldırın\n",
    "df3.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "\n",
    "df3['label'] = df3['label'].apply(lambda x: 0 if x == 'hiçbiri' else 1)\n",
    "\n",
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "H8g5eND1Jhcx"
   },
   "outputs": [],
   "source": [
    "sentences = df['text'].tolist() + df2['text'].tolist() + df3['tweet'].tolist()\n",
    "labels = df['label'].tolist() + df2['label'].tolist() + df3['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93t7_MGKKihv",
    "outputId": "5781b7cc-f2f1-4cb2-a1b7-21a5281c57aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78583"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78583"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(sentences, labels, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght training sentences 62866, lenght testing sentences 15717\n",
      "Lenght training label 62866, lenght testing labels 15717\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lenght training sentences {len(training_sentences)}, lenght testing sentences {len(testing_sentences)}\")\n",
    "print(f\"Lenght training label {len(training_labels)}, lenght testing labels {len(testing_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "3u8UB0MCkZ5N"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "GrAlWBKf99Ya"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "FufaT4vlkiDE"
   },
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(units=hp.Int('neurons_layer_1', min_value=64, max_value=512, step=32),\n",
    "                              activation=hp.Choice('activation_layer_1', values=['relu', 'tanh'])),\n",
    "        tf.keras.layers.Dropout(rate=hp.Float('dropout_layer_1', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        tf.keras.layers.Dense(units=hp.Int('neurons_layer_5', min_value=24, max_value=64, step=12),\n",
    "                              activation=hp.Choice('activation_layer_2', values=['relu', 'tanh'])),\n",
    "        tf.keras.layers.Dense(units=hp.Int('neurons_layer_2', min_value=24, max_value=64, step=12),\n",
    "                              activation=hp.Choice('activation_layer_2', values=['relu', 'tanh'])),\n",
    "        tf.keras.layers.Dropout(rate=hp.Float('dropout_layer_2', min_value=0.0, max_value=0.5, step=0.1)),\n",
    "        tf.keras.layers.Dense(units=hp.Int('neurons_layer_3', min_value=16, max_value=32, step=16),\n",
    "                              activation=hp.Choice('activation_layer_3', values=['relu', 'tanh'])),\n",
    "        tf.keras.layers.Dense(units=hp.Int('neurons_layer_4', min_value=16, max_value=32, step=8),\n",
    "                              activation=hp.Choice('activation_layer_2', values=['relu', 'tanh'])),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Get optimizer from hyperparameters\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCSUcSOnXU6o",
    "outputId": "96cb3032-ed17-4495-aa8e-7b9de6184201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner) (2.32.2)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner) (2024.6.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.1 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 30.7/129.1 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------ -------------------------- 41.0/129.1 kB 667.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 112.6/129.1 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 129.1/129.1 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Oiap0emXH89",
    "outputId": "eb85290a-58d7-4a97-8137-9adb5bcfa98d"
   },
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch, BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKND_iFqU9OQ",
    "outputId": "2615d6be-21bc-4efc-abf5-5bb38de5d64d"
   },
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    create_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='my_tuning_dir_rsearch',\n",
    "    project_name='nlp_model_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGebOYLMVASp",
    "outputId": "8397304b-3a8b-4b23-8832-792dc6a84a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 14s]\n",
      "val_accuracy: 0.8898645043373108\n",
      "\n",
      "Best val_accuracy So Far: 0.9023350477218628\n",
      "Total elapsed time: 00h 13m 04s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(training_padded, training_labels,\n",
    "             epochs=10,\n",
    "             validation_data=(testing_padded, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner[bayesian] in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner[bayesian]) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner[bayesian]) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner[bayesian]) (2.32.2)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner[bayesian]) (1.0.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner[bayesian]) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from keras-tuner[bayesian]) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner[bayesian]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner[bayesian]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner[bayesian]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from requests->keras-tuner[bayesian]) (2024.6.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from scikit-learn->keras-tuner[bayesian]) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from scikit-learn->keras-tuner[bayesian]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nezih\\anaconda3\\envs\\yzup\\lib\\site-packages (from scikit-learn->keras-tuner[bayesian]) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner[bayesian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_tuning_dir_b\\nlp_model_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner_b = BayesianOptimization(\n",
    "    create_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='my_tuning_dir_b',\n",
    "    project_name='nlp_model_tuning'\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 30s]\n",
      "val_accuracy: 0.9012534022331238\n",
      "\n",
      "Best val_accuracy So Far: 0.9037348031997681\n",
      "Total elapsed time: 00h 12m 47s\n"
     ]
    }
   ],
   "source": [
    "tuner_b.search(training_padded, training_labels, epochs=50, validation_data=(testing_padded, testing_labels), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T5Xd0YtLVEcK",
    "outputId": "a542198b-0df4-4a10-cc32-8b53499e0f2b"
   },
   "outputs": [],
   "source": [
    "best_hps_b = tuner_b.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_r = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "JWd9TQMcVMq1"
   },
   "outputs": [],
   "source": [
    "model_b = tuner.hypermodel.build(best_hps_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = tuner.hypermodel.build(best_hps_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "dqCR73TiV3mS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1965/1965 [==============================] - 9s 4ms/step - loss: 0.4118 - accuracy: 0.8200 - val_loss: 0.3326 - val_accuracy: 0.8723\n",
      "Epoch 2/100\n",
      "1965/1965 [==============================] - 9s 4ms/step - loss: 0.2740 - accuracy: 0.8961 - val_loss: 0.4016 - val_accuracy: 0.8631\n",
      "Epoch 3/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.2403 - accuracy: 0.9105 - val_loss: 0.2900 - val_accuracy: 0.8894\n",
      "Epoch 4/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.2212 - accuracy: 0.9192 - val_loss: 0.2846 - val_accuracy: 0.8948\n",
      "Epoch 5/100\n",
      "1965/1965 [==============================] - 9s 4ms/step - loss: 0.2070 - accuracy: 0.9259 - val_loss: 0.2768 - val_accuracy: 0.8983\n",
      "Epoch 6/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1936 - accuracy: 0.9304 - val_loss: 0.2948 - val_accuracy: 0.8972\n",
      "Epoch 7/100\n",
      "1965/1965 [==============================] - 9s 4ms/step - loss: 0.1844 - accuracy: 0.9345 - val_loss: 0.2966 - val_accuracy: 0.8958\n",
      "Epoch 8/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1794 - accuracy: 0.9369 - val_loss: 0.3115 - val_accuracy: 0.8946\n",
      "Epoch 9/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1743 - accuracy: 0.9387 - val_loss: 0.3136 - val_accuracy: 0.8849\n",
      "Epoch 10/100\n",
      "1965/1965 [==============================] - 9s 4ms/step - loss: 0.1677 - accuracy: 0.9429 - val_loss: 0.2940 - val_accuracy: 0.9027\n",
      "Epoch 11/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1655 - accuracy: 0.9437 - val_loss: 0.3116 - val_accuracy: 0.9003\n",
      "Epoch 12/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1621 - accuracy: 0.9449 - val_loss: 0.3107 - val_accuracy: 0.8975\n",
      "Epoch 13/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1565 - accuracy: 0.9471 - val_loss: 0.3033 - val_accuracy: 0.9004\n",
      "Epoch 14/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1534 - accuracy: 0.9488 - val_loss: 0.3525 - val_accuracy: 0.8838\n",
      "Epoch 15/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1498 - accuracy: 0.9505 - val_loss: 0.3548 - val_accuracy: 0.9014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24802a3d670>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_b.fit(training_padded, training_labels, epochs=100, validation_data=(testing_padded, testing_labels), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.4268 - accuracy: 0.8100 - val_loss: 0.3639 - val_accuracy: 0.8558\n",
      "Epoch 2/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.2789 - accuracy: 0.8939 - val_loss: 0.2861 - val_accuracy: 0.8909\n",
      "Epoch 3/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.2416 - accuracy: 0.9092 - val_loss: 0.2791 - val_accuracy: 0.8924\n",
      "Epoch 4/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.2176 - accuracy: 0.9210 - val_loss: 0.3050 - val_accuracy: 0.8895\n",
      "Epoch 5/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.2051 - accuracy: 0.9252 - val_loss: 0.2950 - val_accuracy: 0.8857\n",
      "Epoch 6/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.1936 - accuracy: 0.9288 - val_loss: 0.2765 - val_accuracy: 0.8999\n",
      "Epoch 7/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.1816 - accuracy: 0.9326 - val_loss: 0.2889 - val_accuracy: 0.9004\n",
      "Epoch 8/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.1725 - accuracy: 0.9353 - val_loss: 0.2862 - val_accuracy: 0.9020\n",
      "Epoch 9/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.1614 - accuracy: 0.9379 - val_loss: 0.2976 - val_accuracy: 0.8934\n",
      "Epoch 10/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.1539 - accuracy: 0.9399 - val_loss: 0.3585 - val_accuracy: 0.8638\n",
      "Epoch 11/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1471 - accuracy: 0.9420 - val_loss: 0.2904 - val_accuracy: 0.9062\n",
      "Epoch 12/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1423 - accuracy: 0.9430 - val_loss: 0.3112 - val_accuracy: 0.9055\n",
      "Epoch 13/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1334 - accuracy: 0.9458 - val_loss: 0.3296 - val_accuracy: 0.9060\n",
      "Epoch 14/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1317 - accuracy: 0.9467 - val_loss: 0.3213 - val_accuracy: 0.9011\n",
      "Epoch 15/100\n",
      "1965/1965 [==============================] - 8s 4ms/step - loss: 0.1247 - accuracy: 0.9490 - val_loss: 0.3657 - val_accuracy: 0.8922\n",
      "Epoch 16/100\n",
      "1965/1965 [==============================] - 7s 4ms/step - loss: 0.1203 - accuracy: 0.9500 - val_loss: 0.3221 - val_accuracy: 0.9058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2487c63f670>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_r.fit(training_padded, training_labels, epochs=100, validation_data=(testing_padded, testing_labels), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "_wGJi5itV4uE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0.07115555]\n",
      " [0.3904998 ]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.04762675]\n",
      " [0.27489117]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Herkese Merhaba\", \"terörist grup öldürüldü\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model_b.predict(padded))\n",
    "print(model_r.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 1s 1ms/step\n",
      "492/492 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_model_b = model_b.predict(testing_padded)\n",
    "y_pred_model_r = model_r.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_model_b_binary = [1 if eleman > 0.6 else 0 for eleman in y_pred_model_b]\n",
    "y_pred_model_r_binary = [1 if eleman > 0.6 else 0 for eleman in y_pred_model_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıflandırma Raporu model_b:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     10344\n",
      "           1       0.93      0.76      0.84      5373\n",
      "\n",
      "    accuracy                           0.90     15717\n",
      "   macro avg       0.91      0.87      0.88     15717\n",
      "weighted avg       0.90      0.90      0.90     15717\n",
      "\n",
      "Doğruluk: 0.8984539034166825\n"
     ]
    }
   ],
   "source": [
    "print(\"Sınıflandırma Raporu model_b:\")\n",
    "print(classification_report(testing_labels, y_pred_model_b_binary))\n",
    "print(\"Doğruluk:\", accuracy_score(testing_labels, y_pred_model_b_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıflandırma Raporu model_r:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     10344\n",
      "           1       0.92      0.77      0.84      5373\n",
      "\n",
      "    accuracy                           0.90     15717\n",
      "   macro avg       0.91      0.87      0.88     15717\n",
      "weighted avg       0.90      0.90      0.90     15717\n",
      "\n",
      "Doğruluk: 0.9001081631354584\n"
     ]
    }
   ],
   "source": [
    "print(\"Sınıflandırma Raporu model_r:\")\n",
    "print(classification_report(testing_labels, y_pred_model_r_binary))\n",
    "print(\"Doğruluk:\", accuracy_score(testing_labels, y_pred_model_r_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfDt1hmYkiys",
    "outputId": "c3df0dd6-e1f9-41f5-e4a4-24ad2f809727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 448)               7616      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 448)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 60)                26940     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 60)                3660      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 60)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                1952      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200,713\n",
      "Trainable params: 200,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 250, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               4352      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 60)                15420     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 36)                2196      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 36)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                1184      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 24)                792       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,969\n",
      "Trainable params: 183,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_r.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DTKQFf1kkyc",
    "outputId": "199d6b41-e353-4ff2-99f9-60fa229ec13f"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(kernel='rbf', C=1.5, gamma='scale')\n",
    "svc_model.fit(training_padded, training_labels)\n",
    "y_pred_svc = svc_model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıflandırma Raporu SVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.80     10344\n",
      "           1       0.70      0.15      0.24      5373\n",
      "\n",
      "    accuracy                           0.69     15717\n",
      "   macro avg       0.69      0.56      0.52     15717\n",
      "weighted avg       0.69      0.69      0.61     15717\n",
      "\n",
      "Doğruluk: 0.6868995355347712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"Sınıflandırma Raporu SVC:\")\n",
    "print(classification_report(testing_labels, y_pred_svc))\n",
    "print(\"Doğruluk:\", accuracy_score(testing_labels, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=15)\n",
    "rf_model.fit(training_padded, training_labels)\n",
    "y_pred_rf = rf_model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıflandırma Raporu RF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89     10344\n",
      "           1       0.89      0.64      0.74      5373\n",
      "\n",
      "    accuracy                           0.85     15717\n",
      "   macro avg       0.86      0.80      0.82     15717\n",
      "weighted avg       0.85      0.85      0.84     15717\n",
      "\n",
      "Doğruluk: 0.8477444804988229\n"
     ]
    }
   ],
   "source": [
    "print(\"Sınıflandırma Raporu RF:\")\n",
    "print(classification_report(testing_labels, y_pred_rf))\n",
    "print(\"Doğruluk:\", accuracy_score(testing_labels, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(training_padded, training_labels)\n",
    "y_pred_nb = nb_model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıflandırma Raporu NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73     10344\n",
      "           1       0.44      0.35      0.39      5373\n",
      "\n",
      "    accuracy                           0.63     15717\n",
      "   macro avg       0.57      0.56      0.56     15717\n",
      "weighted avg       0.61      0.63      0.61     15717\n",
      "\n",
      "Doğruluk: 0.6261373035566584\n"
     ]
    }
   ],
   "source": [
    "print(\"Sınıflandırma Raporu NB:\")\n",
    "print(classification_report(testing_labels, y_pred_nb))\n",
    "print(\"Doğruluk:\", accuracy_score(testing_labels, y_pred_nb))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
